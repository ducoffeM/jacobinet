{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6861b9bd-cf15-4b71-b2eb-51ea9d325347",
   "metadata": {},
   "source": [
    "# From Keras to ONNX: A Complete Guide to Jacobinet Backward Model Serialization\n",
    "\n",
    "In this tutorial, we demonstrate how to serialize a Jacobinet model using Keras native serialization tools. We also show how to export Jacobinet models into the ONNX format, which is essential for industrial end-to-end pipelines.\n",
    "\n",
    "**Why this matters**:\n",
    "- **Portability**: ONNX models can be used across different platforms and frameworks.\n",
    "- **Interoperability**: The serialized Keras model can be reloaded and validated, ensuring no corruption occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678812c-97a1-4a5f-9773-75500027abe2",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f614ce-beeb-4f99-91e3-90a256a6e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"onnx\"\n",
    "    !{sys.executable} -m pip install \"onnxruntime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ed914-b098-4798-a927-d1ae6bd85454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb7341-3cca-4439-b136-30b70cc96fe4",
   "metadata": {},
   "source": [
    "## Step 1: Define the Forward Model\n",
    "\n",
    "We will create a simple feedforward neural network with the following architecture:\n",
    "- **Input**: Shape `(3,)`\n",
    "- **Dense Layer**: 10 units + ReLU activation\n",
    "- **Dense Layer**: 1 unit (output layer)\n",
    "\n",
    "This model will be used as the basis for computing the Jacobian later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97018fa3-30dc-44fe-a2a6-1d6b7cc031f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Build the forward model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(10, input_shape=(3,), name=\"Dense1\"),\n",
    "        Activation(\"relu\", name=\"ReLU1\"),\n",
    "        Dense(1, name=\"Output\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate a forward pass to initialize model weights\n",
    "_ = model(Input((3,)))\n",
    "\n",
    "# Display model summary\n",
    "print(\"### Forward Model Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de2533-dd60-42ca-b8d4-331030d6d6b9",
   "metadata": {},
   "source": [
    "## Step 2: Compute the Backward Model Using Jacobinet\n",
    "\n",
    "Jacobinet allows us to compute a **backward model** that represents the gradient of the output with respect to the input. This is key for understanding the chain rule in neural networks, fundamental to backpropagation.\n",
    "\n",
    "We will now compute the backward model using the `clone_to_backward` function from Jacobinet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece774-c952-4cbb-9e82-ec9ae0c5786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Jacobinet and convert the forward model to a backward model\n",
    "import jacobinet\n",
    "from jacobinet import clone_to_backward\n",
    "\n",
    "backward_model = clone_to_backward(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ce2ce-5d48-4fa3-81df-f7300b04a18f",
   "metadata": {},
   "source": [
    "We can save this backward model using Keras' native saving and loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbe280-cf43-46b2-a272-dae4743fe741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the backward model using Keras serialization\n",
    "import keras\n",
    "\n",
    "keras.models.save_model(backward_model, \"my_backward_model.keras\")\n",
    "\n",
    "# Load the backward model\n",
    "my_loaded_backward_model = keras.models.load_model(\"my_backward_model.keras\")\n",
    "\n",
    "# Generate random test input and gradient\n",
    "import numpy as np\n",
    "\n",
    "random_input = np.random.rand(3)[None].astype(\"float32\")  # Batch size 1, 3 features\n",
    "random_grad = np.ones((1, 1), dtype=\"float32\")\n",
    "\n",
    "# Check for model consistency (ensure loaded model matches original model)\n",
    "assert np.allclose(\n",
    "    backward_model.predict([random_input, random_grad]),\n",
    "    my_loaded_backward_model.predict([random_input, random_grad]),\n",
    "), \"Loaded backward model does not match original!\"\n",
    "\n",
    "keras_jacobian = backward_model.predict([random_input, random_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77dfb8-b9cf-4433-b274-1d4646500d38",
   "metadata": {},
   "source": [
    "## Step 3: Export Backward Model to ONNX\n",
    "\n",
    "**Why ONNX?**\n",
    "- ONNX allows the model to be run on different platforms and inference engines.\n",
    "- This enables industrial-level interoperability and simplifies deployment.\n",
    "\n",
    "**Requirements**: \n",
    "- `torch` and `onnx` libraries are required for exporting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ba3c3-89da-4ab6-9bb9-4c87653e2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a Torch wrapper around the Keras model\n",
    "\n",
    "\n",
    "class Keras2Torch(nn.Module):\n",
    "    def __init__(self, keras_model):\n",
    "        super().__init__()\n",
    "        self.keras_model = keras_model\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z = self.keras_model([x, y])\n",
    "        return z\n",
    "\n",
    "\n",
    "# Wrap the Keras backward model in the PyTorch interface\n",
    "torch_model = Keras2Torch(backward_model)\n",
    "\n",
    "# Generate random input and gradient for Torch\n",
    "torch_input = torch.randn(1, 3)  # Batch size 1, 3 features\n",
    "torch_grad = torch.ones(1, 1)  # Gradient size matches the output\n",
    "\n",
    "# Run a forward pass to ensure no errors\n",
    "_ = torch_model(torch_input, torch_grad)\n",
    "\n",
    "# Export the backward model to ONNX format\n",
    "torch.onnx.export(\n",
    "    torch_model,  # Model to export\n",
    "    (torch_input, torch_grad),  # Model inputs (as a tuple)\n",
    "    \"backward_model_torch.onnx\",  # File name to save as\n",
    "    input_names=[\"input_x\", \"input_grad\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_x\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },  # Handle batch size changes\n",
    ")\n",
    "\n",
    "print(\"ONNX model successfully exported as 'backward_model_torch.onnx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50cbc4-e20b-4155-8c69-79d6a847eb7c",
   "metadata": {},
   "source": [
    "## Step 4: Validate the ONNX Model\n",
    "\n",
    "We can use `onnx` to load and check the model, ensuring there is no corruption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac377ba2-b862-432c-b2aa-0ded87e06bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the exported ONNX model\n",
    "onnx_model = onnx.load(\"backward_model_torch.onnx\")\n",
    "\n",
    "# Check the model for errors\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model is valid!\")\n",
    "except Exception as e:\n",
    "    print(\"ONNX model validation failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ab46e-d668-429c-8cfd-0e72ba12d83d",
   "metadata": {},
   "source": [
    "## Step 5: Inference Using ONNX Runtime\n",
    "\n",
    "To ensure the exported model works as expected, we'll run inference on the ONNX model using **ONNX Runtime**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce31f9-c7b4-4d6d-b35e-d0862c6f170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Create an ONNX runtime session\n",
    "ort_sess = ort.InferenceSession(\"backward_model_torch.onnx\")\n",
    "\n",
    "# Print the input and output names of the ONNX graph\n",
    "print(\"Input names:\", [input.name for input in ort_sess.get_inputs()])\n",
    "print(\"Output names:\", [output.name for output in ort_sess.get_outputs()])\n",
    "\n",
    "# Run inference using the same random inputs\n",
    "onnx_inputs = {\"input_x\": random_input, \"input_grad\": random_grad}\n",
    "onnx_jacobian = ort_sess.run(None, onnx_inputs)\n",
    "\n",
    "# Compare ONNX inference with the Keras jacobian\n",
    "assert np.allclose(\n",
    "    onnx_jacobian[0], keras_jacobian, atol=1e-5\n",
    "), \"ONNX inference does not match Keras!\"\n",
    "\n",
    "print(\n",
    "    f\"ONNX Inference Successful! Predicted: {onnx_jacobian[0]}, Keras Prediction: {keras_jacobian}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353b857-c3e7-45a6-8440-034771f4b159",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the ONNX Model\n",
    "\n",
    "You can visualize the structure of the ONNX model using **Netron**, which makes it easy to debug and understand model structure.\n",
    "\n",
    "**Installation**:\n",
    "```bash\n",
    "pip install netron\n",
    "```\n",
    "\n",
    "**Visualization**:\n",
    "```bash\n",
    "netron -b backward_model_torch.onnx\n",
    "```\n",
    "\n",
    "Netron will open a web page showing the full model graph, its layers, and data flow.\n",
    "\n",
    "---\n",
    "\n",
    "With this process, you have a full industrial-grade pipeline for converting Jacobinet models to ONNX. This allows you to use your model in various environments, ensuring high standards for ARP compliance and reproducibility.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
