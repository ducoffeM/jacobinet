{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c42a5b9-00fb-4b5f-8be0-50cb6efb431b",
   "metadata": {},
   "source": [
    "# Jacobinet Tutorial: Computing and Visualizing Gradients Using Backward Models in Keras\n",
    "\n",
    "In this tutorial, we'll build a simple neural network in Keras and then use the *Jacobinet* library to compute the gradient (Jacobian) of the output with respect to the input. We'll visualize both the forward and backward models and explore how the chain rule applies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9beb33-f9dd-4303-a636-5f546ff69661",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f5fe3-482c-4b71-a3c6-e94485b77d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    # missing imports IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e453-6564-4ef0-a5e6-4bf417a022cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b828b76-4648-40a0-bbaf-50a7b70f5857",
   "metadata": {},
   "source": [
    "## Step 1: Define the Forward Model\n",
    "\n",
    "We'll create a simple feedforward neural network with the following architecture:\n",
    "- Dense Layer (10 units) + ReLU activation\n",
    "- Dense Layer (1 unit) - Output layer\n",
    "\n",
    "The model takes a single input of shape `(1,)` and outputs a single value.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "\n",
    "# Build the forward model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(10, input_shape=(1,), name='Dense1'),\n",
    "        Activation('relu', name='ReLU1'),\n",
    "        Dense(1, name='Output'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "print(\"### Forward Model Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d049cdb-8ea2-441b-bc1d-0bc82a6de1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(10, input_shape=(3,), name=\"Dense1\"),\n",
    "        Activation(\"relu\", name=\"ReLU1\"),\n",
    "        Dense(2, name=\"Output\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model(Input((3,)))\n",
    "\n",
    "# Display model summary\n",
    "print(\"### Forward Model Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bad2b7-11f1-47fe-94fa-b0421a606f17",
   "metadata": {},
   "source": [
    "## Visualize the forward model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ba7d1-1e89-4b31-95f4-b7388946f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_img_file = \"./model_dense.png\"\n",
    "keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "from IPython.display import HTML, Image, display\n",
    "\n",
    "display(\n",
    "    HTML('<div style=\"text-align: center;\"><img src=\"{}\" width=\"400\"/></div>'.format(dot_img_file))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c14446-db34-4489-b888-01a3f27fe991",
   "metadata": {},
   "source": [
    "# Step 2: Compute the Backward Model Using JacoBinet\n",
    "\n",
    "*Jacobinet* allows us to compute a backward model that represents the gradient of the output with respect to the input. This is key to understanding the chain rule in neural networks, which is fundamental in backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d898a-8480-441a-99eb-bae8c2d797e2",
   "metadata": {},
   "source": [
    "## Import JacoBinet library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d4d4a-aa3f-4989-8ee6-3b673cba7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jacobinet\n",
    "from jacobinet import clone_to_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67409b0-3fef-40f0-9c64-f4c60c0c3bec",
   "metadata": {},
   "source": [
    "## Get the backward model using JacoBinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101b5c1-2f9e-4759-99e3-4a74c9cf9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_model = clone_to_backward(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e194b77-93fd-4530-a9d0-3b7e08843cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4260f5-a7c2-43c2-b0ac-e5f1c816d14c",
   "metadata": {},
   "source": [
    "## Display backward model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af77554-e243-457b-856c-d8efd7c89533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Backward Model Summary\")\n",
    "backward_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cca506-dca3-4b9f-8f49-adf1b7395ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_img_file_backward = \"./model_dense_backward.png\"\n",
    "keras.utils.plot_model(\n",
    "    backward_model, to_file=dot_img_file_backward, show_shapes=True, show_layer_names=True\n",
    ")\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<div style=\"text-align: center;\"><img src=\"{}\" width=\"800\"/></div>'.format(\n",
    "            dot_img_file_backward\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2729e-c31e-412c-bcee-35ee2b59f4ef",
   "metadata": {},
   "source": [
    "## Step 3: Understanding the Chain Rule in Neural Networks\n",
    "\n",
    "In a neural network, the **chain rule** is fundamental for propagating gradients backward, from the output layer to the input layer. It allows us to compute the gradient of the loss with respect to each parameter by combining partial derivatives at each layer. Specifically, The gradient given the input  can be split using the chain rule along any latent dimension *h* given the following expression:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x_i} \n",
    "$$\n",
    "\n",
    "In this tutorial, the loss function is simply the identity function applied to the single output of the network, meaning the output itself acts as the loss $ L(y) = y$. This simplification helps illustrate how the gradients flow through the network without additional complexity from the loss computation itself.\n",
    "\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial x_i}$: Gradient of the loss \\(L\\) with respect to the input \\(x_i\\).\n",
    "- $\\frac{\\partial L}{\\partial h} $: Gradient of the loss with respect to a latent dimension \\(h\\).\n",
    "- $\\frac{\\partial h}{\\partial x_i}$: Gradient of the latent ouput \\(h\\) with respect to the input \\(x_i\\).\n",
    "\n",
    "### How the Backward Model Works\n",
    "\n",
    "The **backward model** computes the **Jacobian matrix**, which represents all the partial derivatives of the output with respect to the input. This is crucial in backpropagation since it helps propagate the gradients back through the network.\n",
    "\n",
    "In simpler terms, backpropagation computes how much the network's input contributed to the final loss by applying the chain rule across all layers.\n",
    "\n",
    "### Visualizing the Flow of Gradients\n",
    "\n",
    "The forward model computes the predictions. The backward model, on the other hand, tells us how sensitive the output is to changes in the input by tracing gradients backward through the layers.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Using the Jacobinet library, we have successfully visualized the backward propagation model. This backward model helps us understand how gradients flow through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2ac58-b3cf-4a67-8735-3b9a0db02f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62370c81-c188-49ba-9d1e-73fe2525eea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
