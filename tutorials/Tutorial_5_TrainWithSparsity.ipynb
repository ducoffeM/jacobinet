{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622c4ff2-3c75-46a1-8737-513a35ed7962",
   "metadata": {},
   "source": [
    "# Tutorial: Training Neural Networks with Sparse Input Decision Using Jacobinet\n",
    "This tutorial provides a step-by-step guide to building and training a neural network with sparse input decision support using the *Jacobinet* library, which is built on top of Keras. We will explore key concepts, implement the neural network, train it, and visualize the sparsity of the gradient to understand its implications for robustness and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a3e8a-46db-4fff-b4fc-d7b8c29c85f8",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9327d1-37a9-481c-b3af-faa0f587a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    !{sys.executable} -m pip install \"matplotlib\"\n",
    "    # missing imports IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81f54-af3b-4469-82fb-96036af03124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8a621-914a-44e5-876b-e43f66f4d774",
   "metadata": {},
   "source": [
    "## 1. Why Sparse Support is Useful\n",
    "Before diving into the code, letâ€™s understand why sparse support matters.\n",
    "\n",
    "- Robustness: Sparse gradients focus only on the most important features of the input, ignoring irrelevant noise. This makes the network less sensitive to small perturbations or adversarial attacks.\n",
    "- Interpretability: Sparse gradients help identify which parts of the input are the most influential in making a decision. This enhances the model's interpretability, as we can visualize which pixels (in image classification) contribute the most to predictions.\n",
    "\n",
    "\n",
    "These two properties are crucial in high-stakes applications like healthcare, finance, and autonomous systems, where robustness and explainability are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4164e-570e-4b9f-a4ee-408cad1392bc",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dd65e-820e-411e-a37d-df36d72d590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras jacobinet numpy matplotlib\n",
    "# warning for mac users if .fit raise an error set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd48088c-7796-4d95-9352-de2426753631",
   "metadata": {},
   "source": [
    "## 3. Loading and Preprocessing the Data\n",
    "We will use the MNIST dataset, a collection of 28x28 grayscale handwritten digit images. Here, we load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5018ea-1625-459c-bac9-4d1e9f97dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST data and split it into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale the images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Reshape images to have an additional channel dimension (1, 28, 28)\n",
    "x_train = np.expand_dims(x_train, 1)\n",
    "x_test = np.expand_dims(x_test, 1)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb96ab-37ce-43cd-bd62-74319e225f50",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The MNIST data is scaled to the range [0,1] to normalize pixel values.\n",
    "We expand the dimensions to (1, 28, 28) to match the input shape required for a convolutional neural network (CNN).\n",
    "Labels are converted to one-hot vectors for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94131f31-d124-463d-96b4-19751d91f175",
   "metadata": {},
   "source": [
    "## 4. Building the Neural Network\n",
    "We define a CNN using Keras Sequential API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a665b7f-2228-45cf-ab1a-89e6fea5716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(1, 28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3)),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3)),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        # layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "train_model = Sequential(model.layers + [layers.Activation(\"softmax\")])\n",
    "train_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "train_model.fit(x_train, y_train, batch_size=128, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d45f0-d14f-4448-8e6c-89acc383073b",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "Convolutional layers extract patterns from images using filters.\n",
    "MaxPooling layers reduce spatial dimensions, making computation more efficient.\n",
    "Flatten layer transforms the 2D feature maps into a 1D vector.\n",
    "Dense layer produces 10 outputs, one for each digit class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038bb1b-06d6-4558-a726-5b608b8ce847",
   "metadata": {},
   "source": [
    "## 5. Introducing Jacobinet for Sparse Gradients\n",
    "Here, we use Jacobinet to compute and constraint the sparsity of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660764b-e8d3-49c1-82c5-bef90d71949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.ops as K\n",
    "\n",
    "# keeping the k lowest absolute value\n",
    "\n",
    "\n",
    "class Keep_K_LowestValues(layers.Layer):\n",
    "    def __init__(self, cardinality=1, **kwargs):\n",
    "        super(Keep_K_LowestValues, self).__init__(**kwargs)\n",
    "        self.cardinality = keras.Variable(cardinality, trainable=False)\n",
    "\n",
    "    def call(self, inputs_):\n",
    "        inputs_ = K.sort(K.abs(K.reshape(inputs_, (-1, 784))), axis=-1)\n",
    "        outputs = inputs_[:, : K.cast(self.cardinality, \"int\")]\n",
    "        return outputs\n",
    "\n",
    "    def compute_input_shape(input_shape):\n",
    "        return (1, self.cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e5d5d-9c08-40f4-bb00-9d6d55378ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jacobinet\n",
    "import keras.ops as K\n",
    "from jacobinet import clone_to_backward\n",
    "\n",
    "# Create the backward model to compute gradients w.r.t input\n",
    "backward_model = clone_to_backward(model)\n",
    "\n",
    "# Inputs for images and labels\n",
    "data = layers.Input((1, 28, 28))\n",
    "label = layers.Input((10,))\n",
    "\n",
    "gradient = backward_model([data, label])\n",
    "layer_cut = Keep_K_LowestValues(cardinality=50)\n",
    "cut_gradient = layer_cut(gradient)\n",
    "sparse_gradient = K.sum(cut_gradient, axis=-1)\n",
    "max_gradient = K.max(cut_gradient, axis=-1)\n",
    "\n",
    "# New model to optimize both accuracy and gradient sparsity\n",
    "new_model = keras.models.Model(\n",
    "    [data, label], [layers.Activation(\"softmax\")(model(data)), sparse_gradient, max_gradient]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea616a-e1bf-4a03-bea1-3704992f3a77",
   "metadata": {},
   "source": [
    "We create a Keras callback that gradually increases the number of zero components enforced on the input gradient throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62033d86-2fd5-42dd-906a-87cf984600cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class SparsityConstraint(Callback):\n",
    "    def __init__(self, cardinality, inc, max_sparsity_support):\n",
    "        self.cardinality = cardinality\n",
    "        self.inc = inc\n",
    "        self.max_sparsity_support = max_sparsity_support\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 3 == 0 and epoch:\n",
    "            new_value = np.minimum(self.cardinality.numpy() + self.inc, self.max_sparsity_support)\n",
    "            self.cardinality.assign(new_value)\n",
    "\n",
    "\n",
    "sparse_callback = SparsityConstraint(layer_cut.cardinality, 100, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e712d-1d13-4e38-94dd-4c11d395b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with combined losses (accuracy + gradient sparsity constraints)\n",
    "new_model.compile(\n",
    "    loss=[\"categorical_crossentropy\", \"mse\", \"mse\"],\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", \"mse\", \"mse\"],\n",
    "    loss_weights=[1, 100, 1000],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a0698-5e2f-4d4b-9578-180270dc9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "new_model.fit(\n",
    "    [x_train, y_train],\n",
    "    [y_train, 0 * y_train[:, 1], 0 * y_train[:, 1]],\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[sparse_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89da442-831c-4b74-a65b-a1079274fc67",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "Backward gradients: Jacobinet computes the gradient of the model's output w.r.t the input.\n",
    "Sparse constraint: We retain only the lowest 400 elements in the gradient, considering absolute value \n",
    "to enforce sparsity on.\n",
    "Multi-objective training: The loss is a combination of cross-entropy (for classification) and sparsity loss (to promote sparse gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a69e6-4aa8-4c03-9b20-8d7cd92fdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate([x_train, y_train], [y_train, 0 * y_train[:, :1], 0 * y_train[:, :1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4be49-dc62-49d8-bed2-c8b390bd8174",
   "metadata": {},
   "source": [
    "## 6. Visualizing Gradient Sparsity\n",
    "After training, we visualize which parts of the image influence predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca64eea-570e-42fa-8531-221df199a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "print(model.predict(x_test[index : index + 1])[0].argmax(), y_test[index].argmax())\n",
    "img_saliency = backward_model.predict([x_test[index : index + 1], y_test[index : index + 1]])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(np.clip(np.abs(img_saliency[0, 0]), 1e-7, 100), cmap=\"Grays\")\n",
    "ax2.imshow(x_test[index, 0], cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124acd2c-2717-4a8f-ac2f-df356c8728bb",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The **saliency map** shows which pixels influence the network's prediction.\n",
    "Sparse gradients result in cleaner and more interpretable saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cae9d-96cf-43cf-9972-37f827b5b91a",
   "metadata": {},
   "source": [
    "## 7. Evaluating the Model\n",
    "Finally, we evaluate the model's accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a841d8-3145-4bc4-a64c-7fde309bc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.metrics.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09c0e7-fd79-4023-a372-742d700a44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram for the value\n",
    "gradient_value = np.abs(np.reshape(img_saliency, (784,)))\n",
    "bins = np.linspace(gradient_value.min(), gradient_value.max(), 100)\n",
    "counts, _ = np.histogram(gradient_value, bins=bins)\n",
    "plt.bar(bins[:-1], counts, width=np.diff(bins), color=\"purple\", label=\"Correct\")\n",
    "plt.title(\"Distribution of absolute value of the gradient along the pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917cc7a-ca2e-4583-99f1-787bfa9441e2",
   "metadata": {},
   "source": [
    "This step reports the test accuracy, which tells us how well the model generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6d8ad-7d03-4aea-b07f-8d4fd4910687",
   "metadata": {},
   "source": [
    "8. Analyzing Gradient Sparsity\n",
    "We compute sparsity metrics for every image in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406e26d-00ce-44de-a18f-e408371d1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_saliency = backward_model.predict([x_test, y_test], verbose=0)\n",
    "sparsity_scores = (np.reshape(np.abs(gradient_saliency), (-1, 784)) > 1e-4).sum(-1)\n",
    "\n",
    "# Separate the sparsity scores for correct and incorrect predictions\n",
    "good_prediction = np.argmax(model.predict(x_test), -1) == np.argmax(y_test, -1)\n",
    "\n",
    "# Plot histogram of sparsity scores\n",
    "n_bins = 20\n",
    "bins = np.linspace(sparsity_scores.min(), sparsity_scores.max(), n_bins + 1)\n",
    "\n",
    "pos_counts, _ = np.histogram(sparsity_scores[good_prediction], bins=bins)\n",
    "neg_counts, _ = np.histogram(sparsity_scores[~good_prediction], bins=bins)\n",
    "\n",
    "plt.bar(bins[:-1], pos_counts, width=np.diff(bins), color=\"green\", label=\"Correct\")\n",
    "plt.bar(\n",
    "    bins[:-1], neg_counts, width=np.diff(bins), bottom=pos_counts, color=\"red\", label=\"Incorrect\"\n",
    ")\n",
    "plt.xlabel(\"Sparsity Score\")\n",
    "plt.ylabel(\"Count of Samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6488e-92bb-4d64-b533-e8dea6618a33",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "Sparsity score is the number of non-zero elements in the gradient.\n",
    "We visualize how sparsity correlates with prediction success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247642f7-24de-419d-b4c6-88b936b25c1e",
   "metadata": {},
   "source": [
    "Plot a Heatmap of Sparsity Across All Test Samples\n",
    "You can visualize how sparsity varies across all test samples, showing which samples were harder for the model to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76da1e-0dd5-4011-b08c-e7e6c48f8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a label\n",
    "class_label = 0\n",
    "index_label = np.where(np.argmax(y_test, -1) == class_label)[0]\n",
    "gradient_class = gradient_saliency[index_label]\n",
    "\n",
    "average_img = np.mean(np.abs(gradient_class), axis=0)\n",
    "\n",
    "epsilon_grad = 1e-4\n",
    "\n",
    "print(\n",
    "    \"Shared zero gradient support across all the class\",\n",
    "    len(np.where(average_img.flatten() < epsilon_grad)[0]),\n",
    ")\n",
    "\n",
    "plt.imshow(average_img[0], cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d828e-e97c-477b-8fac-3b27ad2f131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_all = gradient_saliency[index_label]\n",
    "\n",
    "average_img_all_class = np.mean(np.abs(gradient_all), axis=0)\n",
    "\n",
    "print(\n",
    "    \"Shared zero gradient support across all the class\",\n",
    "    len(np.where(average_img_all_class.flatten() < epsilon_grad)[0]),\n",
    ")\n",
    "\n",
    "plt.imshow(average_img_all_class[0], cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a76ae0-37a2-4e8d-bdc5-cd6790f95a78",
   "metadata": {},
   "source": [
    "## 9. Baseline Comparison\n",
    "\n",
    "Next we made an ablation study by non using sparse gradient regularization during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9d6f4-fe11-4e32-821c-5b7e77b72ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model WITHOUT sparsity constraints\n",
    "\n",
    "baseline_model = keras.models.clone_model(model)\n",
    "\n",
    "train_baseline_model = Sequential(baseline_model.layers + [layers.Activation(\"softmax\")])\n",
    "train_baseline_model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067de35-1fca-4710-af2b-c00484180761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline_model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383203ca-df7b-4efd-a218-69b94b4696db",
   "metadata": {},
   "source": [
    "Then, compare the test accuracy and visualize the saliency maps of the same images. You'll see that the sparse model focuses on more relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2d82d-3245-4ebc-9a69-54a6bdcb1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_model_baseline = clone_to_backward(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e6b46-a13b-4260-ab38-0ae2bf46fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "img_saliency_sparse = backward_model.predict([x_test[index : index + 1], y_test[index : index + 1]])\n",
    "img_saliency_baseline = backward_model_baseline.predict(\n",
    "    [x_test[index : index + 1], y_test[index : index + 1]]\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.imshow(x_test[index, 0], cmap=\"Blues\")\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax2.imshow(np.clip(img_saliency_sparse[0, 0], epsilon_grad, 100) - epsilon_grad, cmap=\"Blues\")\n",
    "ax2.set_title(\"Sparse Model Saliency\")\n",
    "ax3.imshow(np.clip(img_saliency_baseline[0, 0], epsilon_grad, 100) - epsilon_grad, cmap=\"Blues\")\n",
    "ax3.set_title(\"Baseline Model Saliency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5265c-90e1-41d5-8d62-bf4b45ea53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread values\n",
    "gradient_baseline = backward_model_baseline.predict([x_test, y_test], verbose=0)\n",
    "sparse_baseline = (np.reshape(np.abs(gradient_baseline), (-1, 784)) > epsilon_grad).sum(-1)\n",
    "\n",
    "print(\n",
    "    \"less sparse, average non zero element is {} over 784 input features\".format(\n",
    "        np.mean(sparse_baseline)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91591aa6-c588-41a0-b48e-c26b72b0795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread values\n",
    "gradient_baseline = backward_model.predict([x_test, y_test], verbose=0)\n",
    "sparse_baseline = (np.reshape(np.abs(gradient_baseline), (-1, 784)) > epsilon_grad).sum(-1)\n",
    "\n",
    "print(\n",
    "    \"less sparse, average non zero element is {} over 784 input features\".format(\n",
    "        np.mean(sparse_baseline)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b3933-6422-4852-bb63-dbc66f0ffb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_saliency_baseline = backward_model_baseline.predict([x_test, y_test], verbose=0)\n",
    "# Separate the sparsity scores for correct and incorrect predictions\n",
    "good_prediction_baseline = np.argmax(baseline_model.predict(x_test), -1) == np.argmax(y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4746fdb-f881-40a7-b78a-cacc90cb6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_scores_baseline = (np.reshape(np.abs(gradient_saliency_baseline), (-1, 784)) > 1e-4).sum(\n",
    "    -1\n",
    ")\n",
    "# Plot histogram of sparsity scores\n",
    "n_bins = 20\n",
    "bins_baseline = np.linspace(\n",
    "    sparsity_scores_baseline.min(), sparsity_scores_baseline.max(), n_bins + 1\n",
    ")\n",
    "\n",
    "pos_counts_baseline, _ = np.histogram(\n",
    "    sparsity_scores_baseline[good_prediction_baseline], bins=bins_baseline\n",
    ")\n",
    "neg_counts_baseline, _ = np.histogram(\n",
    "    sparsity_scores_baseline[good_prediction_baseline], bins=bins_baseline\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    bins_baseline[:-1],\n",
    "    pos_counts_baseline,\n",
    "    width=np.diff(bins_baseline),\n",
    "    color=\"green\",\n",
    "    label=\"Correct\",\n",
    ")\n",
    "plt.bar(\n",
    "    bins_baseline[:-1],\n",
    "    neg_counts_baseline,\n",
    "    width=np.diff(bins_baseline),\n",
    "    bottom=pos_counts_baseline,\n",
    "    color=\"red\",\n",
    "    label=\"Incorrect\",\n",
    ")\n",
    "plt.xlabel(\"Sparsity Score for a baseline model\")\n",
    "plt.ylabel(\"Count of Samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fec61-176a-4c23-941e-22901a558afa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This tutorial demonstrated how to train a neural network with sparse gradient support using the Jacobinet library. By incorporating gradient sparsity constraints, we observed improvements in robustness, interpretability, and generalization.\n",
    "\n",
    "### Key Takeaways\n",
    "- Robustness and Interpretability: Sparse gradients prioritize the most critical features, naturally enhancing model robustness against adversarial attacks and improving explainability.\n",
    "- Simple Implementation: Jacobinet makes it easy to introduce gradient sparsity constraints into a Keras training pipeline with minimal changes.\n",
    "- Dynamic Sparsity: We showcased how to gradually increase the cardinality constraint during training, giving the model greater flexibility to adapt.\n",
    "- Future Directions: Further exploration could focus on how different cardinality constraints affect robustness, how sparsity compares to L1 regularization, and how it influences resistance to adversarial attacks.\n",
    "\n",
    "### Next Steps\n",
    "To deepen your understanding, try experimenting with:\n",
    "\n",
    "- Cardinality Adjustment: Vary the gradient cardinality constraint and observe how it affects sparsity, accuracy, and robustness.\n",
    "- Visualization: Generate and compare saliency maps for sparse and non-sparse models.\n",
    "- Adversarial Testing: Test the model's resistance to adversarial perturbations and analyze the impact of gradient sparsity on defense capabilities.\n",
    "### Further Reading\n",
    "If you're interested in the theoretical foundations of sparsity, robustness, and interpretability, here are some key resources:\n",
    "\n",
    "* \"Intriguing Properties of Neural Networks\" by Szegedy et al. (2013) â€” A seminal paper that introduced the concept of adversarial robustness.\n",
    "* \"SmoothGrad: Removing Noise by Adding Noise\" (2017) â€” A technique for generating clear and interpretable saliency maps for model explanations.\n",
    "By exploring these topics, you'll gain a deeper understanding of how gradient sparsity can improve the robustness, interpretability, and efficiency of neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f812060-d6f6-4b67-a77e-ddf4f3b50f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
