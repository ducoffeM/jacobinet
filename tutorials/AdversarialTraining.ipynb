{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061d19d8-022d-4b2c-a81e-afd0adbe7026",
   "metadata": {},
   "source": [
    "# Robust Training with Jacobinet and Adversarial Attacks\n",
    "\n",
    "This tutorial demonstrates the use of Jacobinet for robust training in neural networks. \n",
    "Jacobinet allows the backward pass of a neural network to be represented as a neural network with shared weights. \n",
    "\n",
    "**Goals:**\n",
    "- Understand adversarial attacks (FGSM, PGD) and their impact on model robustness.\n",
    "- Use Jacobinet to implement robust training by regularizing against adversarial examples.\n",
    "- Evaluate robustness with AutoAttack for both baseline and robust training.\n",
    "\n",
    "We will:\n",
    "1. Train a baseline model and evaluate its adversarial robustness.\n",
    "2. Train a robust model with adversarial regularization using Jacobinet.\n",
    "3. Compare adversarial success rates for both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e9c5e-41db-4741-9b8b-cb8c07044f54",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "We will use the MNIST dataset for this tutorial. The dataset is normalized to the [0, 1] range and reshaped for compatibility with the convolutional model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9b89b-2e2a-4847-95da-3ec3fc7b44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST data and split it into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale the images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Reshape images to have an additional channel dimension (1, 28, 28)\n",
    "x_train = np.expand_dims(x_train, 1)\n",
    "x_test = np.expand_dims(x_test, 1)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beafcc44-3d81-4d34-836a-a8aa5e0b089c",
   "metadata": {},
   "source": [
    "## Define and Train the Baseline Model\n",
    "\n",
    "We will build a simple Convolutional Neural Network (CNN) using Keras to serve as the baseline model. \n",
    "This model will be trained on MNIST and evaluated for accuracy on clean data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52589ad-076d-4274-98a4-0bb0e013c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Sequential\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(1, 28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10)\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "train_model = Sequential(model.layers + [layers.Activation('softmax')])\n",
    "train_model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "train_model.fit(x_train, y_train, batch_size=128, epochs=2, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ac9dd-b968-4940-aa8b-94542beb8541",
   "metadata": {},
   "source": [
    "## Evaluate Robustness of Baseline Model\n",
    "\n",
    "We use AutoAttack, a strong adversarial attack framework, to test the baseline model's robustness. \n",
    "AutoAttack generates adversarial examples by varying the attack radius (`epsilon`), and we measure the model's accuracy on these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bafe0f-20e8-4313-95d5-38ad7e24cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "import torch\n",
    "\n",
    "# Test robustness at different epsilon values\n",
    "n = 100\n",
    "random_index = np.random.permutation(len(x_test))[:n]\n",
    "adv_acc = []\n",
    "eps_values = [np.round(eps_i, 2) for eps_i in np.linspace(0.01, 0.2, 10)]\n",
    "\n",
    "for eps in eps_values:\n",
    "    print(eps)\n",
    "    auto_attack = torchattacks.attacks.autoattack.AutoAttack(model, eps=eps)\n",
    "    adv_data = auto_attack(torch.Tensor(x_test[random_index]), torch.tensor(y_test[random_index].argmax(-1)))\n",
    "    acc = len(np.where(model.predict(adv_data, verbose=0).argmax(-1) != y_test[random_index].argmax(-1))[0]) / len(random_index) * 100\n",
    "    adv_acc.append(acc)\n",
    "\n",
    "plt.plot(eps_values, adv_acc)\n",
    "plt.title('Distribution of adversarial success rates with baseline training')\n",
    "plt.xlabel('Epsilon (attack radius)')\n",
    "plt.ylabel('Adversarial success rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1e1fd-8f07-499f-a0e2-02160bf6d913",
   "metadata": {},
   "source": [
    "## Robust Training with Jacobinet\n",
    "\n",
    "To improve robustness, we will train a model that outputs predictions for both clean and adversarial examples. \n",
    "Jacobinet is used to create adversarial examples with Projected Gradient Descent (PGD), which are integrated into the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cd14a-69fb-49cc-a3e2-e5bc1f406b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobinet.attacks import get_adv_model\n",
    "\n",
    "pgd_model = get_adv_model(model, loss='logits', epsilon=1e-2, attack='pgd', n_iter=20)\n",
    "\n",
    "x = layers.Input(shape=(1, 28, 28))\n",
    "y = layers.Input((10,))\n",
    "\n",
    "model_adv = keras.models.Model([x, y], [model(x), model(pgd_model([x, y]))])\n",
    "model_adv.compile('adam',\n",
    "                  loss=[keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
    "                  metrics=['accuracy', 'accuracy'], loss_weights=[1, 1])\n",
    "\n",
    "model_adv.fit([x_train, y_train], [y_train, y_train], batch_size=128, epochs=4, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db32bf6-be6b-4d21-8bc6-7220ccef6e1e",
   "metadata": {},
   "source": [
    "## Evaluate Robustness of Robust Model\n",
    "\n",
    "We use AutoAttack again to evaluate the robust model under varying attack radii. \n",
    "This allows us to compare the adversarial success rates of the baseline and robust models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe760126-4d4d-4d0a-9616-a55d5aba2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_acc_robust = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    print(eps)\n",
    "    auto_attack = torchattacks.attacks.autoattack.AutoAttack(model, eps=eps)\n",
    "    adv_data_robust = auto_attack(torch.Tensor(x_test[random_index]), torch.tensor(y_test[random_index].argmax(-1)))\n",
    "    acc = len(np.where(model.predict(adv_data_robust, verbose=0).argmax(-1) != y_test[random_index].argmax(-1))[0]) / len(random_index) * 100\n",
    "    adv_acc_robust.append(acc)\n",
    "\n",
    "plt.plot(eps_values, adv_acc, label='Baseline Training')\n",
    "plt.plot(eps_values, adv_acc_robust, label='Robust Training')\n",
    "plt.title('Adversarial Success Rates: Baseline vs. Robust Training')\n",
    "plt.xlabel('Epsilon (attack radius)')\n",
    "plt.ylabel('Adversarial success rate')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841d6fa-43e3-44e9-b07a-d83b9557a4c3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the use of Jacobinet for robust training against adversarial attacks. Key takeaways include:\n",
    "1. Baseline models are vulnerable to adversarial examples, as shown by the high adversarial success rates.\n",
    "2. Robust training with Jacobinet significantly improves resistance to adversarial attacks.\n",
    "3. This workflow can be extended to other datasets and adversarial attack frameworks.\n",
    "\n",
    "Jacobinet's ability to treat the backward pass as a neural network opens exciting possibilities for research in robustness and adversarial machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8c05a-eee8-4434-9cee-7d219078f0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
